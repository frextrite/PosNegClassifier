{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk to process language data\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow for obvious reasons\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define word lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a lexicon from language \n",
    "def create_lexicon(files):\n",
    "    words_list = []\n",
    "    # iterate over each file\n",
    "    for file in files:\n",
    "        # open the file and create a context\n",
    "        with open(file, 'r') as f:\n",
    "            sentences = f.readlines()\n",
    "            for sentence in sentences:\n",
    "                words = word_tokenize(sentence)\n",
    "                words_list += list(words)\n",
    "    \n",
    "    # create a word count lexicon copy\n",
    "    lexicon_temp = [lemmatizer.lemmatize(i) for i in words_list]\n",
    "    \n",
    "    # extract each lemma and calculate its number of occurences\n",
    "    words_freq = Counter(lexicon_temp)\n",
    "    \n",
    "    \n",
    "    # create final lexicon containing unique lemmatized words\n",
    "    lexicon = []\n",
    "    for word in words_freq:\n",
    "        # discard rare and most used words\n",
    "        if 8000 > words_freq[word] > 20:\n",
    "            lexicon.append(word)\n",
    "    \n",
    "    # return lexicon\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(sample, lexicon, classification):\n",
    "    labels = []\n",
    "    with open(sample, 'r') as f:\n",
    "        sentences = f.readlines()\n",
    "        for line in sentences:\n",
    "            words = word_tokenize(line.lower())\n",
    "            words = [lemmatizer.lemmatize(i) for i in lexicon]\n",
    "            \n",
    "            features = np.zeros((len(lexicon),1))\n",
    "            \n",
    "            for word in words:\n",
    "                word = word.lower()\n",
    "                if word in lexicon:\n",
    "                    index = lexicon.index(word)\n",
    "                    features[index] = 1\n",
    "                \n",
    "            labels.append(list(features))\n",
    "            \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
