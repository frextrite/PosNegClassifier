{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pos Neg Classifier\n",
    "##### Simple Classifier which classifies if the entered sentence is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk to process language data\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Counter\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow for obvious reasons\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy for data manipulation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "random_state = 42\n",
    "pos_one_hot = [1.0, 0.0]\n",
    "neg_one_hot = [0.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define word lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our data files\n",
    "files = ['./dataset/pos.txt', './dataset/neg.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create words list\n",
    "words_list = []\n",
    "\n",
    "for file in files:\n",
    "    # open the file and create a context\n",
    "    with open(file, 'r') as f:\n",
    "        # read file\n",
    "        sentences = f.readlines()\n",
    "        for sentence in sentences:\n",
    "            # tokenize words\n",
    "            words = word_tokenize(sentence)\n",
    "            words_list += list(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize words\n",
    "lemmatized_words_list = [lemmatizer.lemmatize(i) for i in words_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract each lemma and calculate its number of occurences\n",
    "words_freq = Counter(lemmatized_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1073"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create lexicon containing unique lemmatized words\n",
    "lexicon = []\n",
    "for word in words_freq:\n",
    "    # discard rare and most used words\n",
    "    if 8000 > words_freq[word] > 20:\n",
    "        lexicon.append(word)\n",
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features list\n",
    "X = []\n",
    "# create labels list\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update feature and labels list for pos.txt\n",
    "with open(files[0], 'r') as f:\n",
    "    sentences = f.readlines()\n",
    "    for line in sentences:\n",
    "        words = word_tokenize(line.lower())\n",
    "        words = [lemmatizer.lemmatize(i) for i in words]\n",
    "\n",
    "        features = np.zeros((len(lexicon)))\n",
    "\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word in lexicon:\n",
    "                index = lexicon.index(word)\n",
    "                features[index] += 1\n",
    "            \n",
    "        X.append(list(features))\n",
    "        y.append(pos_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update feature and labels list for neg.txt\n",
    "with open(files[1], 'r') as f:\n",
    "    sentences = f.readlines()\n",
    "    for line in sentences:\n",
    "        words = word_tokenize(line.lower())\n",
    "        words = [lemmatizer.lemmatize(i) for i in words]\n",
    "\n",
    "        features = np.zeros((len(lexicon)))\n",
    "\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word in lexicon:\n",
    "                index = lexicon.index(word)\n",
    "                features[index] += 1\n",
    "            \n",
    "        X.append(list(features))\n",
    "        y.append(neg_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert lists to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "from sklearn.utils import shuffle\n",
    "X, y = shuffle(X, y, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.08, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras to build our model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9809 samples, validate on 853 samples\n",
      "Epoch 1/3\n",
      "9809/9809 [==============================] - 4s 383us/step - loss: 0.6298 - acc: 0.6495 - val_loss: 0.5288 - val_acc: 0.7503\n",
      "Epoch 2/3\n",
      "9809/9809 [==============================] - 3s 285us/step - loss: 0.4858 - acc: 0.7662 - val_loss: 0.5391 - val_acc: 0.7491\n",
      "Epoch 3/3\n",
      "9809/9809 [==============================] - 3s 284us/step - loss: 0.3712 - acc: 0.8316 - val_loss: 0.5631 - val_acc: 0.7608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f07d9b605f8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build our sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# add a dense layer\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# add a dense layer\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# add a dense layer\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# add a dense layer\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "# add a softmax layer\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# fit data into the model\n",
    "model.fit(X_train, y_train, epochs=3, validation_data=[X_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model accuracy: 76.08%\n",
    "##### More Data = Higher Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1024)              1099776   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 4,250,626\n",
      "Trainable params: 4,250,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52989733, 0.47010267]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test custom prediction\n",
    "line = 'You are doing good fam! Keep it up!'\n",
    "words = word_tokenize(line.lower())\n",
    "words = [lemmatizer.lemmatize(i) for i in words]\n",
    "\n",
    "_features = np.zeros((len(lexicon)))\n",
    "\n",
    "for word in words:\n",
    "    word = word.lower()\n",
    "    if word in lexicon:\n",
    "        index = lexicon.index(word)\n",
    "        _features[index] += 1\n",
    "_features = np.reshape(_features, (1,1073))\n",
    "model.predict(_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Above sentence is classified as a positive one with 52% confidence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
